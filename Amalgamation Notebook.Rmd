---
title: "Amalgamation Notebook"
output: html_notebook
---

# ST310 Final Project

## Introduction

## Data Preparation

```{r}
# Import data here as data
data <- read_csv("Final_310_dataset.csv")
# Most of initial code is from EDA
```

```{r}
library(dplyr)
library(tidymodels)
```

```{r}
set.seed(17)
```

```{r}
# Categorical variables
categorical_vars <- c("ST004D01T", "ST005Q01TA", "ST007Q01TA", "ST011Q04TA",
                      "ST011Q06TA", "ST012Q09NA", "EFFORT1", "IMMIG", "ST034Q06TA",
                      "ST176Q03IA", "ST175Q01IA", "ST022Q01TA", "ST012Q02TA", "ST016Q01NA" 
                      )
data <- data %>%
  mutate(across(all_of(categorical_vars), as.factor))
```

```{r}
# Numeric variables
numeric_vars <- c("PV1MATH","PV1READ","PV1SCIE", "ESCS", "AGE", "COMPETE", "GFOFAIL")
data <- data %>%
  mutate(across(all_of(numeric_vars), as.numeric))
```

### Splitting Data into Train and Test

```{r}
data_split <- initial_split(data, prop = 0.7)
data_split
```

```{r}
# Look at data
data_split %>% 
  training() %>% 
  glimpse()
```

```{r}
data_recipe <- training(data_split) %>% 
  recipe(PV1MATH ~.) %>% 
  step_novel(all_nominal_predictors()) %>% # To handle new levels
  step_unknown(all_nominal_predictors()) %>% # To crate new unknown level for missing or rare
  step_dummy(all_nominal_predictors()) %>% # Creates dummy variables for categorical 
  step_corr(all_numeric_predictors()) %>% # Removes highly correlated
  step_center(all_numeric_predictors(), - all_outcomes()) %>% # Centers all numeric mean zero
  step_scale(all_numeric_predictors(), -all_outcomes()) %>% # Standardizes sd of 1
  
  prep()

```

```{r}
data_recipe
```

```{r}
# Execute for test data

data_testing <- data_recipe %>% 
  bake(testing(data_split))

glimpse(data_testing)
```

```{r}
data_training <- juice(data_recipe)

glimpse(data_training)
```

## Base models

### Main Base Model: Linear regression

The initial base models uses only a few predictors
```{r}
# Training the model
data_lm <- linear_reg() %>%  # From parsnip
  set_engine("lm") %>% 
  fit(PV1MATH ~ AGE + PV1READ + ESCS, data = data_training)
# Includes age, reading score, and socioeconomic status.
```

```{r}
# Predictions
predict(data_lm, data_testing)
```

```{r}
# Getting stats
data_lm %>% 
  predict(data_testing) %>% 
  bind_cols(data_testing) %>% 
  metrics(truth = PV1MATH, estimate = .pred)
```
From this baseline, the R-Squared is 0.69, and the RMSE is 57.47, which we will take as a baseline.

### Experimental Linear Regression: Our own Implementation of Gradient Descent

A different data format is needed for this implementation, so the data is prepared here, from the original prepped data.
```{r}
# Prepare data for running own gradient descent

# Training data
X_train <- as.matrix(select(data_training, -PV1MATH))  
Y_train <- pull(data_training, PV1MATH)  

# Testing data
X_test <- as.matrix(select(data_testing, -PV1MATH))
Y_test <- pull(data_testing, PV1MATH)

  
```

In this section, we manually implemented the gradient descent algorithm to a simplified linear regression model. We chose this version of the linear regression model so that the gradient descent algorithm is simpler to implement. The additional assumption in the model is that the estimate passes through the origin, as we removed the intercept term.

\$ Y = X\beta \$, and hence \$ \hat Y = X \hat\beta\$.

The loss used here is the Mean Squared Error (MSE) loss and is defined by:

\$ MSE = 1/n \sum \^n\_{i=1} (y_i - \hat y_i)\^2\$

where $n$ is the number of observations, $y_i$ is the true value of the target variable for the $i_{th}$ observation, and $\hat y_i$ is the corresponding estimate.

In order to apply the gradient descent algorithm, the gradient of the loss function is taken with respect to the parameters \$ \hat\beta \$. As there is a closed form solution for this formula, we used:

\$ \nabla \_\beta MSE = -2/n \space X\^T (Y - X\beta) \$

as the gradient.

```{r}
# Defining loss and gradient of loss functions wrt beta
mse_loss <- function(X, Y, beta) {
  return(mean((Y - X %*% beta)^2)) # Formula above
}

gradient_mse <- function(X, Y, beta) {
  n <- length(Y) 
  gradient <- -2 / n * t(X) %*% (Y - X %*% beta) # Formula above
  return(gradient)
}
```

#### Gradient Descent Methodology

In this section, we defined a function that runs the gradient descent algorithm for the model described above and a variable step size using the Barzilai–Borwein (BB) method. The code below is inspired by the class notebook, adapted for linear regression with key changes, described here.

First, the algorithm is initialized. As the BB method requires to keep track of the beta values from two previous iterations, the initial beta is generated using random values from the normal distribution. A step is taken in the direction of the first calculated gradient using a fixed initial step size. The second beta is then obtained, and the corresponding loss is calculated. A step counter is also initialized.

Next, the main algorithm is executed within a while loop. The difference from consecutive gradients is calculated for the BB method, and the step size is obtained. The betas are shifted, and the main gradient descent update step is taken using the BB method's learning rate:

\$ \beta\_{t+1} \leftarrow \beta\_t - \|\gamma\_t\|\nabla{\beta_t} MSE\$

where $t$ refers to the previous iteration.

With the new beta, the loss is recalculated, and the loop repeats itself until termination. There are two ways in which the algorithm can terminate. In case of convergence, the algorithm stops when the difference between the two previous losses is smaller than a predetermined threshold. On the other hand, if the threshold is not obtained the max-steps absolute limit is enforced.

```{r}
# Own gradient descet function definition
own_gd <- function(X, Y, max_steps = 100, tol = 1e-6, initial_step_size = 0.1){


  beta_prev2 <- rnorm(ncol(X))
  grad_prev2 <- gradient_mse(X, Y, beta_prev2) 
  
  beta_prev1 <- beta_prev2 - initial_step_size * grad_prev2 
  grad_prev1 <- gradient_mse(X, Y, beta_prev1) # Gradient at first step
  
  previous_loss <- mse_loss(X, Y, beta_prev2)# MSE Loss
  next_loss <- mse_loss(X, Y, beta_prev1) 
  
  steps <- 1
  
  
  
  while (abs(previous_loss - next_loss) > tol) {
    
    # Calculating step size with BB method
    grad_diff <- grad_prev1 - grad_prev2
    step_BB <- sum((beta_prev1 - beta_prev2) * grad_diff) / sum(grad_diff^2)
    
    # Update beta
    beta_prev2 <- beta_prev1
    beta_prev1 <- beta_prev1 - abs(step_BB) * grad_prev1 
    
    # Update gradients
    grad_prev2 <- grad_prev1
    grad_prev1 <- gradient_mse(X, Y, beta_prev1)
    
    # Update loss
    previous_loss <- next_loss
    next_loss <- mse_loss(X, Y, beta_prev1)
    
    # Print loss, update step count, control loop
    if (round(steps/5) == steps/5) print(previous_loss)
    steps <- steps + 1
    if (steps > max_steps) break
  }

  return(list(beta = beta_prev1, loss = next_loss))
}

```

```{r}
# Here we run the algorithm
result <- own_gd(X_train, Y_train)

#print(result$beta) 
#print(result$loss)  
```

```{r}
# Here we use the result to make predictions
predictions <- X_test %*% result$beta # Using formula

# Workflow code
own_gd_predictions <- data_testing %>%
  bind_cols(predictions = as.vector(predictions)) %>% 
  metrics(truth = PV1MATH, estimate = predictions)

own_gd_predictions
```

## Interpretable Models

Two additional interpretable models were implemented and tested to evaluate how well different modelling approaches predict students' mathematics scores (PV1MATH). 
A Generalised Additive Model (GAM), 
A Decision Tree

Each model's performance was evaluated using Root Mean Squared Error (RMSE), which provides an interpretable measure of prediction error in the same units as the outcome variable.


### GAMs
The GAM was constructed using the“mgcv” engine and included several continuous predictors modelled using smoothed terms. These predictors were socioeconomic status (ESCS), reading score (PV1READ), science score (PV1SCIE), age (AGE), confidence in competition (COMPETE), and fear of failure (GFOFAIL). The smoothing functions (s()) enabled the model to capture non-linear relationships between these features and the target variable.

The RMSE was approximately 51.83, indicating that the average prediction error was under 52 points. This suggests that the GAM may have effectively captured complex, non-linear data structures that contribute to math performance.



```{r}
gam_model <- gen_additive_mod() %>%
  set_engine("mgcv") %>%
  set_mode("regression")

gam_fit <- fit(
  gam_model,
  PV1MATH ~ s(ESCS) + s(PV1READ) + s(PV1SCIE) + s(AGE) + s(COMPETE) + s(GFOFAIL), data = training(data_split)
)

gam_model <- gen_additive_mod() %>%
  set_engine("mgcv") %>%
  set_mode("regression")

predictions <- predict(gam_fit, new_data = testing(data_split))
```

```{r}
test_data <- testing(data_split)

gam_preds <- predict(gam_fit, new_data = test_data) %>%
  bind_cols(test_data %>% dplyr::select(PV1MATH))

gam_rmse <- rmse(gam_preds, truth = PV1MATH, estimate = .pred)
gam_rmse
```

### Decision Tree

The decision tree model was implemented using the “rpart” engine, which builds a tree by recursively splitting the data based on feature values to minimise error. This model used all available predictors in the dataset but imposed a rigid, hierarchical structure where decisions are made at specific thresholds.

After fitting the tree on the training set and evaluating it on the test set, the resulting RMSE was 57.00, higher than that of the GAM. This indicates that although the decision tree could capture some non-linearity and interactions, it lacked the flexibility of the GAM and may have either underfitted or overfitted the data depending on the tree's structure and depth.


```{r}
tree_model <- decision_tree() %>%
  set_engine("rpart") %>%
  set_mode("regression")

tree_workflow <- workflow() %>%
  add_model(tree_model) %>%
  add_formula(PV1MATH ~ .)

tree_fit <- fit(tree_workflow, data = training(data_split))

predict(tree_fit, new_data = testing(data_split))
```

```{r}
test_data <- testing(data_split)

tree_preds <- predict(tree_fit, new_data = test_data) %>%
  bind_cols(test_data %>% dplyr::select(PV1MATH)) 

tree_rmse <- rmse(tree_preds, truth = PV1MATH, estimate = .pred)
tree_rmse
```

## High Dim Models
For our high dimensional model, we ran a Lasso regression using the tidymodels framework. 
We chose Lasso because it shrinks less important predictors to zero. This leads to an interpretable model.

We originally had 22 features, so we expanded this to 522 to make this relatively high dim. We did this through preprocessing steps such as interactions (step_interact) and removing highlight correlated variables. 

Initally, we faced some errors when trying interactions, as we were facing errors that we had run out of RAM space on each of our laptops. So instead of interacting every single variable with each other we filtered the variables to just interact the ones that started with "ST0" as most of our variables started with this and still expanded to a decent p>n scenario for high dim.

We picked 10 fold cross validation to tune for the best lambda. The lambda that was selected as the optimal was penalty = 0.0264 which was found using RMSE.

The final model was evaluated on the unseen test data. Lasso shrinked many coefficients to zero and only 378 out of 522 features were in the final model. 
Key retained features include academic indicators like PV1READ, PV1SCIE, and ESCS, which align with theoretical expectations. 
### Additional Data Preparation
```{r}
highdim_recipe <- training(data_split) %>% 
  recipe(PV1MATH ~.) %>% 
  step_novel(all_nominal_predictors()) %>% # To handle new levels
  step_unknown(all_nominal_predictors()) %>% # To crate new unknown level for missing or rare
  step_dummy(all_nominal_predictors()) %>% # Creates dummy variables for categorical 
  step_corr(all_numeric_predictors()) %>% # Removes highly correlated
  step_center(all_numeric_predictors(), - all_outcomes()) %>% # Centers all numeric mean zero
  step_zv(all_predictors()) %>%
  step_scale(all_numeric_predictors(), -all_outcomes()) %>% # Standardizes sd of 1
  step_interact(terms = ~ starts_with("ST0"):starts_with("ST0"))


# Prep the recipe
prep_recipe_highdim <- prep(highdim_recipe)

# Check the number of columns after preprocessing
baked_data_highdim <- bake(prep_recipe_highdim, new_data = NULL)

# View the number of predictors (excluding outcome)
num_predictors_highdim <- ncol(baked_data_highdim) - 1
cat("Number of predictors after preprocessing:", num_predictors_highdim, "\n")

```

### Lasso Regression
```{r}
# Lasso model with tuning for penalty
lasso <- linear_reg(
  penalty = tune(), #lambda
  mixture = 1   # L1 for lasso
  ) %>%
  set_engine("glmnet") %>%
  set_mode("regression")

# Workflow using formula
lasso_wf <- workflow() %>%
  add_model(lasso) %>%
  add_recipe(highdim_recipe)

cv_folds <- vfold_cv(training(data_split), v = 10)

# Same tuning grid
lambda_grid <- grid_regular(penalty(), levels = 20)

# Tune the lasso model using cross-validation
lasso_res <- tune_grid(
  lasso_wf,
  resamples = cv_folds,
  grid = lambda_grid,
  control = control_grid(save_pred = TRUE)
)

# Select best lambda based on RMSE
best_lasso <- select_best(lasso_res, metric = "rmse")
print(best_lasso)

# Finalize workflow with best lambda
final_lasso <- finalize_workflow(lasso_wf, best_lasso)

# Evaluate on test set
final_fit <- last_fit(final_lasso, split = data_split)

# Metrics
collect_metrics(final_fit)

# Plot predictions
collect_predictions(final_fit) %>%
  ggplot(aes(x = .pred, y = PV1MATH)) +
  geom_point(alpha = 0.4) +
  geom_abline(color = "red") +
  labs(title = "Lasso: Predicted vs Actual Math Scores")

# Optional: see which variables were kept
final_fit %>%
  extract_workflow() %>%
  tidy() %>%
  filter(estimate != 0)
```

### Ridge Regression

Ridge regression was used as the second type of high dimensional model. We used interactions to get 522 features from 22, making it high dimensional. The goal was predictive accuracy and it was good at dealing with many correlated predictors which were present such as PVSCIE and PVREAD. Unlike lasso which removed the variables that were shrunk to 0, ridge reression it shrinks predictors towards 0 showing they are less important but keeps them in the model.

```{r}
# Ridge model specification (L2 penalty)
ridge_model <- linear_reg(
  penalty = tune(),
  mixture = 0  # Ridge
) %>%
  set_engine("glmnet") %>%
  set_mode("regression")

# Workflow
ridge_wf <- workflow() %>%
  add_recipe(highdim_recipe) %>%
  add_model(ridge_model)

# Cross-validation folds
cv_folds <- vfold_cv(data_training, v = 10)

# Grid for tuning lambda
lambda_grid <- grid_regular(penalty(), levels = 20)

# Tune model
ridge_res <- tune_grid(
  ridge_wf,
  resamples = cv_folds,
  grid = lambda_grid,
  control = control_grid(save_pred = TRUE)
)

# Select best lambda
best_ridge <- select_best(ridge_res, metric = "rmse")
print(best_ridge)

# Finalize and fit on full training data
final_ridge <- finalize_workflow(ridge_wf, best_ridge)

# Final model evaluation on test data
final_ridge_fit <- last_fit(final_ridge, split = data_split)

# Metrics
collect_metrics(final_ridge_fit)

# Plot predictions
collect_predictions(final_ridge_fit) %>%
  ggplot(aes(x = .pred, y = PV1MATH)) +
  geom_point(alpha = 0.4) +
  geom_abline(color = "red") +
  labs(title = "Ridge: Predicted vs Actual Math Scores")

# Optional: check coefficient shrinkage (will not zero-out in ridge)
final_ridge_fit %>%
  extract_workflow() %>%
  tidy() %>%
  arrange(desc(abs(estimate)))
```





## Prediction Based Models

### Boosted Tree

```{r}
boost_model <- boost_tree(
  trees = 1000,         
  tree_depth = 6,        
  learn_rate = 0.01,     
  loss_reduction = 0,    
  sample_size = 1,       
  mtry = NULL,          
  stop_iter = 10        
) %>%
  set_engine("xgboost") %>%
  set_mode("regression")

boost_workflow <- workflow() %>%
  add_model(boost_model) %>%
  add_formula(PV1MATH ~ .)

boost_fit <- fit(boost_workflow, data = training(data_split))

test_data <- testing(data_split)

boost_preds <- predict(boost_fit, new_data = test_data) %>%
  bind_cols(test_data %>% dplyr::select(PV1MATH))

boost_rmse <- rmse(boost_preds, truth = PV1MATH, estimate = .pred)
boost_rmse
```

### Random Forest

```{r}
forest_model <- rand_forest(
  trees = 1000,  
  mtry = NULL,    
  min_n = 5           
) %>%
  set_engine("ranger") %>% # This is apparently more efficient than random forest
  set_mode("regression")

forest_workflow <- workflow() %>%
  add_model(forest_model) %>%
  add_formula(PV1MATH ~ .)

forest_fit <- fit(forest_workflow, data = training(data_split))

test_data <- testing(data_split)

forest_preds <- predict(forest_fit, new_data = test_data) %>%
  bind_cols(test_data %>% dplyr::select(PV1MATH))

forest_rmse <- rmse(forest_preds, truth = PV1MATH, estimate = .pred)
forest_rmse

```


## Summary Table

Model | R-Squared | RMSE
Baseline LR | 0.69 | 57.47
GD LR | 0.76 | 463.90
GAM | NA | 51.83
Decision Tree | NA | 57.00




